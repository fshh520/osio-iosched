osio
=========

--------
概述
--------
osio是一个简单的io调度算法，适用于SSD。
osio像noop一样，不会对io请求按扇区进行排序。因此osio适用于以flash为基础的存储器(ssd、sd卡、u盘)，不适用于机械硬盘。
但是和noop不同的是，osio会像deadline一样，将io请求按方向分类，并考虑调度的公平性。
osio将io请求分为3个方向：读、同步写、异步写。
和deadline一样，osio优先满足读请求，但osio不会饿死写请求，当同步写或异步写的饥饿程度超过了（大于）它们的
饥饿线（sync_write_starved_line和async_write_starved_line）才会像读一样去考虑写请求。

为什么读请求没有同步和异步之分呢？
内核将所有读请求都视为同步的，可以从下面的代码中得到应证：
*| include/linux/blkdev.h
*| static inline bool rw_is_sync(unsigned int rw_flags)
*| {
*|	return !(rw_flags & REQ_WRITE) || (rw_flags & REQ_SYNC);
*| }
*|
*| static inline bool rq_is_sync(struct request *rq)
*| {
*| 	return rw_is_sync(rq->cmd_flags);
*| }
事实上，大部分写请求都是异步的。（例如，复制文件时的大量写请求是异步的，mount/umount文件系统时发生的一次写请求是同步的^_^）

---------
编译与安装
---------
$ make
$ sudo make install

或

$ make deb
将得到 osio-ioscheduler_0.8-1_all.deb
$ sudo dpkg -i osio-ioscheduler_0.8-1_all.deb

这份代码是在kernel-3.12.5的基础上编写而成的，并不能保证在其他版本的内核中能顺利编译与运行。
内核中的API是不断变化的，其他版本的内核可能需要根据其API进行修改。将来会增加这部分代码。

---------
如何使用
---------
先获得root权限
$ su

插入osio的内核模块
# modprobe osio

查看osio是否已经可用（注：sdX有可能是sda、sdb...）
# cat /sys/block/sdX/queue/scheduler
如果可用，将在输出结果出现osio，就像这样
noop [deadline] cfq osio

修改调度算法
# echo osio > /sys/block/sdX/queue/scheduler
# cat /sys/block/sdX/queue/scheduler
可以得到下面的信息
noop deadline cfq [osio]

--------------
可供调整的参数
--------------
所有参数都在 /sys/block/sdX/queue/iosched/
async_write_starved_line: 异步写请求的饥饿线，默认值:5 最小值:1 最大值:65535
sync_write_starved_line: 同步写请求的饥饿线，默认值:1 最小值:0 最大值:65535
fifo_read_batch: 一批读请求的最大数量，默认值:8 最小值:1 最大值:65535
fifo_async_write_batch: 一批异步写请求的最大数量，默认值:4 最小值:1 最大值:65535
fifo_sync_write_batch: 一批同步写请求的最大数量，默认值:4 最小值:1 最大值:65535

当sync_write_starved_line == async_write_starved_line == 0时，
如果同时存在大量的读，同步写和异步写请求，读请求将一直得不到响应，
因为当进行完一批同步写时，异步写已经处于饥饿状态，而进行完一批异步写时，同步写又处于
饥饿状态，也就是写将一直饥饿。所以读将被饿死。
因此，这里将async_write_starved_line的最小值限定为1。

